{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ""
     ]
    }
   ],
   "source": [
    "'''==图象识别模型CNN、RNN、LSTM、Attention……等 精简解析版=='''\n",
    "# coding=utf-8\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "#读数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('whale/MinistData', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "==CNN卷积神经网络 图象识别 精简骨架==\n",
    "'''\n",
    "\n",
    "def CNN(inputs,output,FLAGS):\n",
    "\n",
    "    #==构造卷积神经网络==\n",
    "    #==第一层==\n",
    "    # 1、权重项初始化 w \n",
    "    w_1 = tf.Variable(tf.truncated_normal([5,5,1,32],stddev=0.1))\n",
    "    #Variable tf生成变量\n",
    "    #truncated_normal 压宿初始化为正态分布值，shape张量（向量加深度）维度  truncate缩短 trun-枝干\n",
    "    #filter滑动窗口大小 shape 5 5 1 32 高 宽 深RGB灰 输出32个特征图度，stddev标准差\n",
    "    # 2、偏移量初始化 b\n",
    "    b_1 = tf.Variable(tf.random_normal([32],stddev=0.1))#初使化为正态分布随机数\n",
    "    #b_1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "    #生成常数constant 32个，wx+b输入结果是32个特征图神经元，所以w,b都是输出32维度\n",
    "    # 3、第一层卷积 conv,用tf的tf.nn.conv2d()，relu激活函数在这一层加，下一步只用来浓缩卷积处理好的特征图\n",
    "    #取x整理\n",
    "    x_1 = tf.reshape(inputs,[-1,FLAGS.image_height,FLAGS.image_width,FLAGS.channel])\n",
    "    #reshape特征拉平用于计算 wx 使维度匹配\n",
    "    conv_1 = tf.nn.conv2d(x_1,w_1,strides=[1,1,1,1],padding='SAME')\n",
    "    #conv_1 = tf.nn.relu(conv_1+b_1)\n",
    "    conv_1 = tf.nn.relu(tf.nn.bias_add(conv_1,b_1))\n",
    "    #tf.nn.conv2d(x,w,strides=[1,1,1,1],padding='SAME')+b , SAME不满格时补充\n",
    "    #【卷积的是wx 滑动计算】 5 5 5 5 filter滑动窗口大小，步长 1 1 1 1第次向右向下走一格，结果+b偏移量，再整体relu\n",
    "    # 4、第一层池化 把卷积后的特征图 压缩pooling 浓缩代表（特征群取最大值代表用tf.nn.max_pool()）特征图 pool\n",
    "    pool_1 = tf.nn.max_pool(conv_1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    #ksize = [1,2,2,1]#池化压缩窗口大小2*2 开始的窗口是5*5，strides步长\n",
    "    #前后两个1一般不管，有一个chanel一个batch 不在这两个维度做池化（只是平面的特征压缩，深度和batch不变 设1）\n",
    "    #加dropout\n",
    "    pool_1 = tf.nn.dropout(pool_1,keep_prob=FLAGS.keep_drop)\n",
    "    \n",
    "    #==第二层==\n",
    "    w_2 = tf.Variable(tf.truncated_normal([5,5,32,64],stddev=0.1))\n",
    "    #这一层的w，图高宽还是5*5，深度不再是一张灰度 而是前面传过来的32个特征图，这次输出64个特征图\n",
    "    #truncated_normal 生成随机数 不是nn的神经网络的函数\n",
    "    b_2 = tf.Variable(tf.random_normal([64],stddev=0.1))\n",
    "    conv_2 = tf.nn.conv2d(pool_1,w_2,strides=[1,1,1,1],padding='SAME')\n",
    "    conv_2 = tf.nn.relu(tf.nn.bias_add(conv_2,b_2))\n",
    "    #这里的输入是上一层最后的输出pool后的结果pool_1\n",
    "    pool_2 = tf.nn.max_pool(conv_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    #加dropout\n",
    "    pool_2 = tf.nn.dropout(pool_2,keep_prob=FLAGS.keep_drop)\n",
    "    \n",
    "    \n",
    "    #==全连接层（全连接，即，此层不卷积，全部参与连接wx+b计算最终输出分类连softmax）==\n",
    "    fc_w_1 = tf.Variable(tf.truncated_normal([7*7*64,1024],stddev=0.1))\n",
    "    #全连接层的输入是一维的，把前一层池化结果拉平，w也要维度匹配,\n",
    "    #这里用了固定的7*7 28*28图经过一层池化压缩变14*14(28/2 2*2池化),经第二次压缩变7*7，再*64个特征图，\n",
    "    #输出是1024维\n",
    "    fc_b_1 = tf.Variable(tf.random_normal([1024],stddev=0.1))\n",
    "    #计算本层wx+b 全连接层全参与计算，上一层pool_2做输入，几个特征图要拉平变一维\n",
    "    flat_pool_2 = tf.reshape(pool_2,[-1,7*7*64])#拉平成只有高宽两元素的，\n",
    "    #-1表式bachsize不管？还是chanel不动？还是跟据另一维自动变？待查\n",
    "    #计算wx+b\n",
    "    fc_1 = tf.nn.relu(tf.add(tf.matmul(flat_pool_2,fc_w_1),fc_b_1))\n",
    "    #tf.matmul相当于np.dot，矩阵乘，做内积，不是对应位置乘\n",
    "    \n",
    "    #==加Dropout（全连接层每个神经元都保留会过拟合，舍弃一些,按比例keep_prob保留）==\n",
    "    #if train == True:#控制是训练还是测试，训练集dropout\n",
    "    fc_1 = tf.nn.dropout(fc_1,keep_prob=FLAGS.keep_drop)   \n",
    "    \n",
    "    #==连softmax分类器层变成类别概率值==\n",
    "    fc_w_2 = tf.Variable(tf.truncated_normal([1024,10],stddev=0.1))\n",
    "    #输入上一个全连接层的输出1024，输出10个分类\n",
    "    fc_b_2 = tf.Variable(tf.random_normal([10],stddev=0.1))#b匹配的是输入，有多少个结果对应就是多少个wx+b个式子，即多少个b\n",
    "    #【卷积计算需要输入float型 常数项b也一样，要设初始值为0.0或0.1】\n",
    "    #【别忘了第二参是shape=】\n",
    "    #softmax计算\n",
    "    #_pred = tf.nn.softmax(tf.add(tf.matmul(fc_1,fc_w_2),fc_b_2))\n",
    "    #【注意：以上不可在外面加softmax，加完直接用tf.nn.softmax_cross_entropy_with_logits算交叉熵损失即可，\n",
    "    #这个类里面已经整合了softmax】\n",
    "    _pred = tf.add(tf.matmul(fc_1,fc_w_2),fc_b_2)\n",
    "    #插播一条：logits log(p/(1-p)) 广义线性模型里的，比值比，多分类，用比值比 logits变成符合线性的多分类\n",
    "    \n",
    "    #==以上两层卷积神经网络构造完成\n",
    "    #以下是算结果的loss，和精度计算==\n",
    "    \n",
    "    #==用平均交叉熵计算loss==\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_pred,labels=output))\n",
    "    #logits分类结果与labels output y做比较求loss 验证准确率\n",
    "    \n",
    "    #==梯度下降优化模型（相当于普通神经网络的反向传播优化的过程），\n",
    "    #用tf.train.AdamOptimizer 梯度下降Adam优化器Optimizer优化，\n",
    "    #作用是寻找全局最优点优化算法，引入二次方梯度校正\n",
    "    #lr学习率==\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(cost)\n",
    "        \n",
    "    #==验证准确率==\n",
    "    _corr=tf.equal(tf.argmax(_pred,1),tf.argmax(output,1))#axis=1比较行中各元素\n",
    "    #预测值argmax最大索引（最大的概率代表它的分类），与真实最大索引equal相等说明预测正确。\n",
    "    #算平均损失\n",
    "    accuracy = tf.reduce_mean(tf.cast(_corr,tf.float32))#cast类型转换 #'float' 换\n",
    "    \n",
    "    return train_op,cost,accuracy#返回训练下降优化函数、损失函数、准确率计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "参数：\n",
      "BATCH_SIZE=16\n",
      "CHANNEL=1\n",
      "EPOCHS=15\n",
      "IMAGE_HEIGHT=28\n",
      "IMAGE_WIDTH=28\n",
      "KEEP_DROP=0.7\n",
      "LR=0.001\n",
      "NUM_CLASSES=10\n",
      "==epoch: 0/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 6.946770 训练准确率 : 0.250000\n",
      "==epoch: 1/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 2.629630 训练准确率 : 0.437500\n",
      "==epoch: 2/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 1.856184 训练准确率 : 0.375000\n",
      "==epoch: 3/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 1.473435 训练准确率 : 0.500000\n",
      "==epoch: 4/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 1.373744 训练准确率 : 0.562500\n",
      "==epoch: 5/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 1.171537 训练准确率 : 0.437500\n",
      "==epoch: 6/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 1.099165 训练准确率 : 0.625000\n",
      "==epoch: 7/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.852211 训练准确率 : 0.625000\n",
      "==epoch: 8/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.900321 训练准确率 : 0.562500\n",
      "==epoch: 9/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.812786 训练准确率 : 0.812500\n",
      "==epoch: 10/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.716849 训练准确率 : 0.687500\n",
      "==epoch: 11/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.751677 训练准确率 : 0.750000\n",
      "==epoch: 12/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.746090 训练准确率 : 0.875000\n",
      "==epoch: 13/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.481871 训练准确率 : 0.750000\n",
      "==epoch: 14/15\n",
      "batch:10/10 16 55000\n",
      "平均损失 : 0.490636 训练准确率 : 0.937500\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#==用CNN做上图点分类任务==\n",
    "\n",
    "#tf.reset_default_graph()#重置图\n",
    "\n",
    "def main(_):\n",
    "    #train=True\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "    #if len(FLAGS) <=4:#【解决二次运行报错定义两次BUG:\n",
    "    #未定义过时（FLAGS长度为4，里面有4个HELP项）才定义，】\n",
    "    #放弃，下次运行时直接点工具栏刷新按钮重启服务再运行即可。\n",
    "    #或像下面这种删除定义的属性再定义\n",
    "    #tf.flags.FLAGS.__delattr__('image_width')\n",
    "    #tf.flags.FLAGS.__delattr__('image_height')\n",
    "    #tf.flags.FLAGS.__delattr__('num_classes')\n",
    "    for key in sorted(FLAGS):\n",
    "        tf.flags.FLAGS.__delattr__(key)#删除定义的所有key,再定义\n",
    "    \n",
    "    #【==tensorflow参数定义，类似于类里的属性定义的作用\n",
    "    #tf三种变量定义与赋值比较:\n",
    "    #flags：定义的直接给下面用的单一值，可以不定义在参数里，直接用等号定义也可以使用；\n",
    "    #Variable：是定义一个格式几维的变量组有初始化的值；\n",
    "    #placeholder：是先占位，再在sess下，feed传值，相当于一个函数传参的作用\n",
    "    #（它也是先有一个格式，但与Variable不同的是没初始化值，后面传进去的）\n",
    "    #(Variable初始化的值，因为是一个函数，所以需要run执行下 \n",
    "    #run的是初始化函数类对象，sess.run(tf.global_variables_initializer())\n",
    "    #我们自己实例化的类对象需要执行或取值时run我们自己取的对象名字，返回值是相应函数的计算结果)\n",
    "    #】==\n",
    "    tf.app.flags.DEFINE_integer('image_width', 28, '图宽')\n",
    "    tf.app.flags.DEFINE_integer('image_height', 28, '图高')\n",
    "    tf.app.flags.DEFINE_integer('num_classes', 10, '分几类')\n",
    "    tf.app.flags.DEFINE_integer('channel',1,'通道RGB 1 灰度图')\n",
    "    tf.app.flags.DEFINE_float('keep_drop',.7,'dropout保留率70%')\n",
    "    tf.app.flags.DEFINE_float('lr',0.001,'学习率')\n",
    "    #tf.app.flags.DEFINE_string('checkpoints','./checkpoints/model.ckpt','模型保存路径')\n",
    "    tf.app.flags.DEFINE_integer('batch_size',16,'一次训练多少图')\n",
    "    tf.app.flags.DEFINE_integer('epochs',15,'训练轮数，所有数据跑一遍为一轮')\n",
    "    \n",
    "    FLAGS.flag_values_dict()\n",
    "\n",
    "    #打印参数\n",
    "    print(\"\\n参数：\")\n",
    "    for key in sorted(FLAGS):\n",
    "        value = FLAGS[key].value\n",
    "        print(\"{}={}\".format(key.upper(),value)) \n",
    "        \n",
    "    #输入输出初始化 x y\n",
    "    inputs = tf.placeholder(dtype=tf.float32,shape=[None,FLAGS.image_width*FLAGS.image_height])\n",
    "    output = tf.placeholder(dtype=tf.int32, shape=[None, FLAGS.num_classes])\n",
    "    \n",
    "    #==使用神经网络CNN函数==\n",
    "    train_op,cost,accuracy = CNN(inputs,output,FLAGS)\n",
    "    \n",
    "    #==tensorflow sess处理执行部分==\n",
    "    \n",
    "    sess_config = tf.ConfigProto(\n",
    "        allow_soft_placement = True,\n",
    "        log_device_placement = True\n",
    "    )\n",
    "    #allow_soft_placement True tf自动选择设备 log_device_placement True 获取tf自动指派的设备并打印\n",
    "    \n",
    "    sess = tf.Session(config=sess_config)\n",
    "    \n",
    "    with sess.as_default():#创建一个默认会话,#类似的with tf.name_scope是命名空间，防止有name的命名冲突\n",
    "        \n",
    "        #实例化保存类\n",
    "        #saver = tf.train.Saver(max_to_keep=1000)#只保留1000次迭代\n",
    "        \n",
    "        #run 变量初始化\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #保存模型,就是sess\n",
    "        #saver.restore(sess,FLAGS.checkpoints)#checkpoints保存路径\n",
    "        \n",
    "        #=开始训练=\n",
    "        # begin train\n",
    "        for epoch in range(FLAGS.epochs):  \n",
    "            print(\"==epoch: %d/%d\"%(epoch,FLAGS.epochs))\n",
    "            #totalBatch = int(mnist.train.num_examples / FLAGS.batch_size)\n",
    "            avg_cost = 0 #平均损失\n",
    "            totalBatch = 10\n",
    "            for k in range(totalBatch):\n",
    "                batch_xs,batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n",
    "                #print(\"==batch_xs.shape==\",batch_xs.shape)\n",
    "                #print(\"==batch_ys.shape==\",batch_ys.shape)\n",
    "                #【sess 处理上面 train_op(梯度下降优化迭代器)#logits分类结果,交差，acc精度等计算结果,\n",
    "                #返回loss值，精确度等\n",
    "                #并feed tensorflow数据，在这一步实现】\n",
    "                #_, network, loss, acc = sess.run([train_op, _pred, cost, accuracy], \n",
    "                #                                 feed_dict={inputs: batch[0], output: batch[1]})\n",
    "                #【可以在一起run，也可以分开，如下】：\n",
    "                feed_dict_v = {inputs: batch_xs, output: batch_ys}\n",
    "                sess.run(train_op,feed_dict=feed_dict_v)\n",
    "                avg_cost += sess.run(cost,feed_dict=feed_dict_v)/totalBatch\n",
    "                acc = sess.run(accuracy,feed_dict=feed_dict_v)\n",
    "                #【需要返回值的函数就要用sess run一下，为placeholder占位的inputs output赋值\n",
    "                #精确度和loss的结果是在这里返回的，要用sess.run一下，返回的是相当于函数执行的结果，不能直接取】\n",
    "            print(\"batch:%d/%d %d %d\"%(k+1,totalBatch,FLAGS.batch_size,mnist.train.num_examples))    \n",
    "            print('平均损失 : %f 训练准确率 : %f'% (avg_cost, acc))\n",
    "        #print('测试准确率:', accuracy.eval({inputs: mnist.test.images, output: mnist.test.labels}))\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【以上做了几个优化，有利于提升模型精确度：\n",
    "#关键处BUG修改：最后一层只需算完add+，即用于 softmax_cross_entropy_with_logits 交叉熵运算，\n",
    "#不要先在外面加一层softmax】\n",
    "#还做了以下几个优化\n",
    "#b，由固定常数变为正态分布随机数random_normal([32],stddev=0.1\n",
    "#b的+用了tf.nn.bias_add 最后一层用了tf.add，因为维度只有一维了\n",
    "#epoch和batch_size的调整之类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "==RNN循环神经网络 图象识别 精简骨架==\n",
    "\n",
    "【持续更新中，请稍假关注】\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
